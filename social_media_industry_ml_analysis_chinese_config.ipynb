{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d10ae4e",
   "metadata": {},
   "source": [
    "# ç¤¾äº¤åª’ä½“è¡Œä¸šçº§åˆ†æï¼ˆä¸­æ–‡æ•™å­¦ç‰ˆ | A+B+C å…¨è¦†ç›– | å«ä¸€é”®å®‰è£…ä¸ç»Ÿä¸€é…ç½®ï¼‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3625cb0",
   "metadata": {},
   "source": [
    "## Chunk 0a â€” ä¸€é”®å®‰è£…ä¾èµ–ï¼ˆå¯é€‰è¿è¡Œï¼‰\n",
    "**ä½œç”¨**ï¼šä¸€é”®å®‰è£…/å‡çº§å…³é”®ä¾èµ–ã€‚å·²å®‰è£…å¯è·³è¿‡ã€‚é‡å¯å†…æ ¸åå†ä»å¤´è¿è¡Œã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2f9a817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "âœ… å®‰è£…å®Œæˆï¼šè‹¥é¦–æ¬¡å®‰è£…/å‡çº§äº†æ ¸å¿ƒåŒ…ï¼Œå»ºè®®é‡å¯å†…æ ¸ï¼ˆKernel â†’ Restart Kernelï¼‰åå†è¿è¡Œåç»­å•å…ƒã€‚\n"
     ]
    }
   ],
   "source": [
    "# === Chunk 0a: ä¸€é”®å®‰è£…ä¾èµ–ï¼ˆå¯é€‰ï¼‰ ===\n",
    "%pip install -q --upgrade pip setuptools wheel\n",
    "\n",
    "# ä¸»æµå»ºæ¨¡ & è§£é‡Š & æ—¶é—´åºåˆ—\n",
    "%pip install -q \\\n",
    "  numpy pandas matplotlib scikit-learn \\\n",
    "  catboost==1.2.5 xgboost==2.1.1 lightgbm==4.5.0 \\\n",
    "  shap==0.46.0 statsmodels==0.14.2 pmdarima==2.0.4\n",
    "\n",
    "# Prophetï¼ˆå¦‚éœ€ä½¿ç”¨ï¼Œå¯èƒ½éœ€è¦åç«¯ CmdStanï¼Œè§ä¸‹æ–¹æ³¨é‡Šï¼‰\n",
    "%pip install -q prophet==1.1.5 cmdstanpy==1.2.4\n",
    "\n",
    "print(\"âœ… å®‰è£…å®Œæˆï¼šè‹¥é¦–æ¬¡å®‰è£…/å‡çº§äº†æ ¸å¿ƒåŒ…ï¼Œå»ºè®®é‡å¯å†…æ ¸ï¼ˆKernel â†’ Restart Kernelï¼‰åå†è¿è¡Œåç»­å•å…ƒã€‚\")\n",
    "\n",
    "# ===== å¯é€‰ï¼šProphet åç«¯ï¼ˆCmdStanï¼‰ä¸€é”®å®‰è£… =====\n",
    "# æŸäº›ç¯å¢ƒä¸‹ Prophet éœ€è¦å…ˆç¼–è¯‘ CmdStanã€‚è”ç½‘è€—æ—¶è¾ƒé•¿ï¼ŒæŒ‰éœ€å¼€å¯ã€‚\n",
    "# è¿è¡Œåå»ºè®®é‡å¯å†…æ ¸ã€‚\n",
    "# from cmdstanpy import install_cmdstan\n",
    "# install_cmdstan()  # å¦‚ç½‘ç»œ/æƒé™å—é™ï¼Œå¯è·³è¿‡ï¼›Chunk 7 ä¼šè‡ªåŠ¨é™çº§åˆ°å…¶ä»–æ–¹æ³•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3df79df",
   "metadata": {},
   "source": [
    "## Chunk 0b â€” ç»Ÿä¸€é…ç½®åŒº\n",
    "**ä½œç”¨**ï¼šé›†ä¸­ç®¡ç†éšæœºç§å­ã€åˆ‡åˆ†æ¯”ä¾‹ã€çˆ†æ¬¾é˜ˆå€¼ã€å€™é€‰æ¨¡å‹åˆ—è¡¨ä¸é€šç”¨è¶…å‚ã€‚åç»­å•å…ƒè‡ªåŠ¨å¼•ç”¨ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18d84677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²è½½å…¥ç»Ÿä¸€é…ç½® CFGï¼š\n",
      "  csv_path: Cleaned_Viral_Social_Media_Trends.csv\n",
      "  date_col: Post_Date\n",
      "  random_state: 42\n",
      "  test_size: 0.25\n",
      "  viral_percentile: 0.9\n",
      "  reg_backends: ['catboost', 'xgboost', 'lightgbm', 'rf']\n",
      "  clf_backends: ['catboost', 'xgboost', 'lightgbm', 'rf']\n",
      "  catboost: {...}\n",
      "  xgboost: {...}\n",
      "  lightgbm: {...}\n",
      "  rf: {...}\n",
      "  force_disable_prophet: False\n",
      "  max_shap_samples: 300\n"
     ]
    }
   ],
   "source": [
    "# === Chunk 0b: ç»Ÿä¸€é…ç½® ===\n",
    "CFG = {\n",
    "    # åŸºç¡€\n",
    "    \"csv_path\": \"Cleaned_Viral_Social_Media_Trends.csv\",\n",
    "    \"date_col\": \"Post_Date\",\n",
    "    \"random_state\": 42,\n",
    "    \"test_size\": 0.25,\n",
    "\n",
    "    # çˆ†æ¬¾é˜ˆå€¼ï¼ˆå¹³å°å†… Top p% è§†ä¸ºçˆ†æ¬¾ï¼‰\n",
    "    \"viral_percentile\": 0.90,   # æ”¹æˆ 0.95 = å‰ 5% è§†ä¸ºçˆ†æ¬¾\n",
    "\n",
    "    # å€™é€‰æ¨¡å‹ä¼˜å…ˆçº§ï¼ˆä»å·¦åˆ°å³å°è¯•ï¼Œè‡ªåŠ¨å›é€€ï¼‰\n",
    "    \"reg_backends\": [\"catboost\", \"xgboost\", \"lightgbm\", \"rf\"],\n",
    "    \"clf_backends\": [\"catboost\", \"xgboost\", \"lightgbm\", \"rf\"],\n",
    "\n",
    "    # é€šç”¨è¶…å‚ï¼ˆæŒ‰éœ€è°ƒæ•´ï¼‰\n",
    "    \"catboost\": {\"depth\": 6, \"learning_rate\": 0.08, \"iterations\": 600, \"verbose\": 0},\n",
    "    \"xgboost\":  {\"n_estimators\": 600, \"learning_rate\": 0.05, \"max_depth\": 6, \"subsample\": 0.9, \"colsample_bytree\": 0.9},\n",
    "    \"lightgbm\": {\"n_estimators\": 700, \"learning_rate\": 0.05, \"num_leaves\": 31},\n",
    "    \"rf\":       {\"n_estimators\": 400},\n",
    "\n",
    "    # å¯é€‰å¼€å…³ï¼ˆé‡åˆ°ç¼–è¯‘/æƒé™é—®é¢˜æ—¶å¾ˆæœ‰ç”¨ï¼‰\n",
    "    \"force_disable_prophet\": False,  # True æ—¶å¼ºåˆ¶å…³é—­ Prophetï¼Œèµ°é™çº§è·¯çº¿\n",
    "    \"max_shap_samples\": 300,         # SHAP å–æ ·æ•°é‡ï¼ˆæ§åˆ¶é€Ÿåº¦ï¼‰\n",
    "}\n",
    "\n",
    "print(\"âœ… å·²è½½å…¥ç»Ÿä¸€é…ç½® CFGï¼š\")\n",
    "for k, v in CFG.items():\n",
    "    print(f\"  {k}: {{...}}\" if isinstance(v, dict) else f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8305a5e5",
   "metadata": {},
   "source": [
    "## Chunk 1 â€” å¯¼å…¥ä¾èµ–ä¸ç¯å¢ƒæ£€æµ‹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35787c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ç¯å¢ƒæ£€æµ‹ï¼š\n",
      "  CatBoost: å¯ç”¨\n",
      "  XGBoost: å¯ç”¨\n",
      "  LightGBM: å¯ç”¨\n",
      "  SHAP: å¯ç”¨\n",
      "  Prophet: å¯ç”¨ | seasonal_decompose: å¯ç”¨\n"
     ]
    }
   ],
   "source": [
    "# === Chunk 1: å¯¼å…¥ä¾èµ–ä¸ç¯å¢ƒæ£€æµ‹ ===\n",
    "import os, sys, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    classification_report, confusion_matrix, silhouette_score\n",
    ")\n",
    "\n",
    "# å»ºæ¨¡æ¡†æ¶æ¢æµ‹\n",
    "try:\n",
    "    from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "    CATBOOST_OK = True\n",
    "except Exception:\n",
    "    CATBOOST_OK = False\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBRegressor, XGBClassifier\n",
    "    XGBOOST_OK = True\n",
    "except Exception:\n",
    "    XGBOOST_OK = False\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "    LGBM_OK = True\n",
    "except Exception:\n",
    "    LGBM_OK = False\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# è§£é‡Šæ€§\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_OK = True\n",
    "except Exception:\n",
    "    SHAP_OK = False\n",
    "\n",
    "# æ—¶é—´åºåˆ—ï¼ˆProphet å¯é€‰ï¼‰\n",
    "PROPHET_OK = False\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "    PROPHET_OK = True\n",
    "except Exception:\n",
    "    try:\n",
    "        from fbprophet import Prophet  # å…¼å®¹è€åŒ…å\n",
    "        PROPHET_OK = True\n",
    "    except Exception:\n",
    "        PROPHET_OK = False\n",
    "\n",
    "# statsmodels å­£èŠ‚åˆ†è§£ï¼ˆé™çº§æ–¹æ¡ˆï¼‰\n",
    "try:\n",
    "    from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "    SM_SEASONAL_OK = True\n",
    "except Exception:\n",
    "    SM_SEASONAL_OK = False\n",
    "\n",
    "# ç»Ÿä¸€é…ç½®å¯¹ Prophet çš„ç¡¬å¼€å…³ï¼ˆé‡åˆ°ç¼–è¯‘/æƒé™é—®é¢˜æ—¶å¼€å¯ Trueï¼‰\n",
    "if CFG.get(\"force_disable_prophet\", False):\n",
    "    PROPHET_OK = False\n",
    "\n",
    "print(\"âœ… ç¯å¢ƒæ£€æµ‹ï¼š\")\n",
    "print(\"  CatBoost:\", \"å¯ç”¨\" if CATBOOST_OK else \"ä¸å¯ç”¨\")\n",
    "print(\"  XGBoost:\", \"å¯ç”¨\" if XGBOOST_OK else \"ä¸å¯ç”¨\")\n",
    "print(\"  LightGBM:\", \"å¯ç”¨\" if LGBM_OK else \"ä¸å¯ç”¨\")\n",
    "print(\"  SHAP:\", \"å¯ç”¨\" if SHAP_OK else \"ä¸å¯ç”¨\")\n",
    "print(\"  Prophet:\", \"å¯ç”¨\" if PROPHET_OK else \"ä¸å¯ç”¨\", \"| seasonal_decompose:\", \"å¯ç”¨\" if SM_SEASONAL_OK else \"ä¸å¯ç”¨\")\n",
    "\n",
    "# å°æç¤ºï¼šè‹¥ Prophet=ä¸å¯ç”¨ æˆ–è¿è¡ŒæŠ¥é”™ï¼Œå¯åœ¨ CFG ä¸­æŠŠ force_disable_prophet è®¾ä¸º Trueï¼Œ\n",
    "# åç»­æ—¶é—´åºåˆ— Chunk ä¼šè‡ªåŠ¨é™çº§åˆ° seasonal_decompose / Auto-ARIMA / ç§»åŠ¨å¹³å‡ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e56b58",
   "metadata": {},
   "source": [
    "## Chunk 2 â€” è¯»å–æ•°æ®ä¸åŸºç¡€æ¸…æ´—\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a0e0e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•°æ®è¡Œæ•°/åˆ—æ•°ï¼š (5000, 11)\n",
      "åˆ—æ¸…å•ï¼š ['Post_ID', 'Post_Date', 'Platform', 'Hashtag', 'Content_Type', 'Region', 'Views', 'Likes', 'Shares', 'Comments', 'Engagement_Level', 'Year', 'Month', 'Weekday', 'Hour', 'Engagement_Rate'] ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post_ID</th>\n",
       "      <th>Post_Date</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Hashtag</th>\n",
       "      <th>Content_Type</th>\n",
       "      <th>Region</th>\n",
       "      <th>Views</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Shares</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Engagement_Level</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Engagement_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Post_1</td>\n",
       "      <td>2022-01-13</td>\n",
       "      <td>TikTok</td>\n",
       "      <td>#Challenge</td>\n",
       "      <td>Video</td>\n",
       "      <td>UK</td>\n",
       "      <td>4163464</td>\n",
       "      <td>339431</td>\n",
       "      <td>53135</td>\n",
       "      <td>19346</td>\n",
       "      <td>High</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.098935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Post_2</td>\n",
       "      <td>2022-05-13</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>#Education</td>\n",
       "      <td>Shorts</td>\n",
       "      <td>India</td>\n",
       "      <td>4155940</td>\n",
       "      <td>215240</td>\n",
       "      <td>65860</td>\n",
       "      <td>27239</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2022</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.074192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Post_3</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>#Challenge</td>\n",
       "      <td>Video</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>3666211</td>\n",
       "      <td>327143</td>\n",
       "      <td>39423</td>\n",
       "      <td>36223</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.109865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Post_ID  Post_Date   Platform     Hashtag Content_Type  Region    Views  \\\n",
       "0  Post_1 2022-01-13     TikTok  #Challenge        Video      UK  4163464   \n",
       "1  Post_2 2022-05-13  Instagram  #Education       Shorts   India  4155940   \n",
       "2  Post_3 2022-01-07    Twitter  #Challenge        Video  Brazil  3666211   \n",
       "\n",
       "    Likes  Shares  Comments Engagement_Level  Year  Month  Weekday  Hour  \\\n",
       "0  339431   53135     19346             High  2022      1        3     0   \n",
       "1  215240   65860     27239           Medium  2022      5        4     0   \n",
       "2  327143   39423     36223           Medium  2022      1        4     0   \n",
       "\n",
       "   Engagement_Rate  \n",
       "0         0.098935  \n",
       "1         0.074192  \n",
       "2         0.109865  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Chunk 2: Load & basic clean ===\n",
    "CSV_PATH = CFG[\"csv_path\"]\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    print(\"âŒ æœªæ‰¾åˆ° CSVï¼š\", CSV_PATH)\n",
    "else:\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    print(\"æ•°æ®è¡Œæ•°/åˆ—æ•°ï¼š\", df.shape)\n",
    "    # æ—¶é—´\n",
    "    dtcol = CFG[\"date_col\"]\n",
    "    if dtcol in df.columns:\n",
    "        df[dtcol] = pd.to_datetime(df[dtcol], errors=\"coerce\")\n",
    "        df[\"Year\"] = df[dtcol].dt.year\n",
    "        df[\"Month\"] = df[dtcol].dt.month\n",
    "        df[\"Weekday\"] = df[dtcol].dt.dayofweek\n",
    "        df[\"Hour\"] = df[dtcol].dt.hour\n",
    "    # äº’åŠ¨ç‡\n",
    "    for c in [\"Views\",\"Likes\",\"Shares\",\"Comments\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    if \"Engagement_Rate\" not in df.columns and all(c in df.columns for c in [\"Views\",\"Likes\",\"Shares\",\"Comments\"]):\n",
    "        df[\"Engagement_Rate\"] = (df[\"Likes\"].fillna(0)+df[\"Shares\"].fillna(0)+df[\"Comments\"].fillna(0)) / df[\"Views\"].replace(0, np.nan)\n",
    "        df[\"Engagement_Rate\"] = df[\"Engagement_Rate\"].fillna(0.0)\n",
    "    print(\"åˆ—æ¸…å•ï¼š\", list(df.columns)[:20], \"...\")\n",
    "    display(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8e3f95",
   "metadata": {},
   "source": [
    "## Chunk 3 â€” å¿«é€Ÿ EDA ä¸å¹³å°å¯¹æ¯”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99570dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å„å¹³å°å¹³å‡æŒ‡æ ‡ï¼š\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Views</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Shares</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Engagement_Rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Platform</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>YouTube</th>\n",
       "      <td>2.545648e+06</td>\n",
       "      <td>258314.0023</td>\n",
       "      <td>50073.0914</td>\n",
       "      <td>25321.3905</td>\n",
       "      <td>0.5064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TikTok</th>\n",
       "      <td>2.515015e+06</td>\n",
       "      <td>244206.7198</td>\n",
       "      <td>51468.2563</td>\n",
       "      <td>24778.6968</td>\n",
       "      <td>0.6387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Twitter</th>\n",
       "      <td>2.506005e+06</td>\n",
       "      <td>245880.1188</td>\n",
       "      <td>50227.7508</td>\n",
       "      <td>24456.8571</td>\n",
       "      <td>0.4654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Instagram</th>\n",
       "      <td>2.404080e+06</td>\n",
       "      <td>257118.2178</td>\n",
       "      <td>50310.9092</td>\n",
       "      <td>24958.1139</td>\n",
       "      <td>0.6621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Views        Likes      Shares    Comments  Engagement_Rate\n",
       "Platform                                                                     \n",
       "YouTube    2.545648e+06  258314.0023  50073.0914  25321.3905           0.5064\n",
       "TikTok     2.515015e+06  244206.7198  51468.2563  24778.6968           0.6387\n",
       "Twitter    2.506005e+06  245880.1188  50227.7508  24456.8571           0.4654\n",
       "Instagram  2.404080e+06  257118.2178  50310.9092  24958.1139           0.6621"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆ†æï¼šå¹³å°å‡å€¼åæ˜ è§„æ¨¡å·®å¼‚ï¼Œä¸ç­‰åŒROIï¼›åç»­å»ºæ¨¡å°†è¿›è¡Œå½’å› ä¸é¢„æµ‹ã€‚\n"
     ]
    }
   ],
   "source": [
    "# === Chunk 3: EDA platform compare ===\n",
    "if 'Platform' in df.columns:\n",
    "    metrics = [c for c in [\"Views\",\"Likes\",\"Shares\",\"Comments\",\"Engagement_Rate\"] if c in df.columns]\n",
    "    if metrics:\n",
    "        eda = df.groupby(\"Platform\")[metrics].mean().round(4).sort_values(metrics[0], ascending=False)\n",
    "        print(\"å„å¹³å°å¹³å‡æŒ‡æ ‡ï¼š\")\n",
    "        display(eda)\n",
    "        print(\"åˆ†æï¼šå¹³å°å‡å€¼åæ˜ è§„æ¨¡å·®å¼‚ï¼Œä¸ç­‰åŒROIï¼›åç»­å»ºæ¨¡å°†è¿›è¡Œå½’å› ä¸é¢„æµ‹ã€‚\")\n",
    "    else:\n",
    "        print(\"âš ï¸ æ— å¯ç”¨æ•°å€¼æŒ‡æ ‡ç”¨äºå¹³å°å¯¹æ¯”ã€‚\")\n",
    "else:\n",
    "    print(\"âš ï¸ ç¼ºå°‘ Platform åˆ—ï¼Œè·³è¿‡å¹³å°å¯¹æ¯”ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56efbd9c",
   "metadata": {},
   "source": [
    "## Chunk 4 â€” ç‰¹å¾/ç›®æ ‡è®¾å®šï¼ˆå›å½’ï¼šé¢„æµ‹äº’åŠ¨ç‡ï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cdad2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å›å½’ä»»åŠ¡ â€” X ç»´åº¦ï¼š (5000, 11)   y æ ·æœ¬ï¼š 5000\n",
      "ç±»åˆ«ç‰¹å¾ï¼š ['Platform', 'Content_Type', 'Region', 'Hashtag']\n",
      "æ•°å€¼ç‰¹å¾ï¼š ['Views', 'Likes', 'Shares', 'Comments', 'Hour', 'Weekday', 'Month']\n"
     ]
    }
   ],
   "source": [
    "# === Chunk 4: Feature/Target for Regression ===\n",
    "target_reg = \"Engagement_Rate\" if \"Engagement_Rate\" in df.columns else None\n",
    "cat_cols = [c for c in [\"Platform\",\"Content_Type\",\"Region\",\"Hashtag\"] if c in df.columns]\n",
    "num_cols = [c for c in [\"Views\",\"Likes\",\"Shares\",\"Comments\",\"Hour\",\"Weekday\",\"Month\"] if c in df.columns]\n",
    "\n",
    "if target_reg is None:\n",
    "    print(\"âŒ æ²¡æœ‰ Engagement_Rateï¼Œæ— æ³•åšå›å½’é¢„æµ‹ã€‚\")\n",
    "else:\n",
    "    if \"Hashtag\" in cat_cols:\n",
    "        freq = df[\"Hashtag\"].astype(str).value_counts()\n",
    "        top = set(freq.head(50).index)\n",
    "        df[\"Hashtag\"] = df[\"Hashtag\"].astype(str).where(df[\"Hashtag\"].isin(top), \"_other_\")\n",
    "\n",
    "    X_reg = df[cat_cols + num_cols].copy()\n",
    "    y_reg = df[target_reg].astype(float).values\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        (\"num\", StandardScaler(), num_cols),\n",
    "    ])\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X_reg, y_reg, test_size=CFG[\"test_size\"], random_state=CFG[\"random_state\"])\n",
    "    print(\"å›å½’ä»»åŠ¡ â€” X ç»´åº¦ï¼š\", X_reg.shape, \"  y æ ·æœ¬ï¼š\", len(y_reg))\n",
    "    print(\"ç±»åˆ«ç‰¹å¾ï¼š\", cat_cols)\n",
    "    print(\"æ•°å€¼ç‰¹å¾ï¼š\", num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bb223d",
   "metadata": {},
   "source": [
    "## Chunk 5 â€” å›å½’æ¨¡å‹å¯¹æ¯”ï¼ˆéµå¾ªç»Ÿä¸€é…ç½® CFGï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22bfc8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š æ¨¡å‹ï¼šCatBoostRegressor\n",
      "  RMSE: 7.263349ï¼ˆè¶Šå°è¶Šå¥½ï¼‰\n",
      "  MAE:  0.309797ï¼ˆè¶Šå°è¶Šå¥½ï¼‰\n",
      "  RÂ²:   0.2822ï¼ˆè¶Šæ¥è¿‘ 1 è¶Šå¥½ï¼‰\n",
      "------------------------------------------------------------\n",
      "ğŸ“Š æ¨¡å‹ï¼šXGBRegressor\n",
      "  RMSE: 7.102914ï¼ˆè¶Šå°è¶Šå¥½ï¼‰\n",
      "  MAE:  0.332713ï¼ˆè¶Šå°è¶Šå¥½ï¼‰\n",
      "  RÂ²:   0.3135ï¼ˆè¶Šæ¥è¿‘ 1 è¶Šå¥½ï¼‰\n",
      "------------------------------------------------------------\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000323 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1097\n",
      "[LightGBM] [Info] Number of data points in the train set: 3750, number of used features: 34\n",
      "[LightGBM] [Info] Start training from score 0.488445\n",
      "ğŸ“Š æ¨¡å‹ï¼šLGBMRegressor\n",
      "  RMSE: 7.394393ï¼ˆè¶Šå°è¶Šå¥½ï¼‰\n",
      "  MAE:  0.419739ï¼ˆè¶Šå°è¶Šå¥½ï¼‰\n",
      "  RÂ²:   0.2560ï¼ˆè¶Šæ¥è¿‘ 1 è¶Šå¥½ï¼‰\n",
      "------------------------------------------------------------\n",
      "ğŸ“Š æ¨¡å‹ï¼šRandomForestRegressor\n",
      "  RMSE: 6.413075ï¼ˆè¶Šå°è¶Šå¥½ï¼‰\n",
      "  MAE:  0.257073ï¼ˆè¶Šå°è¶Šå¥½ï¼‰\n",
      "  RÂ²:   0.4404ï¼ˆè¶Šæ¥è¿‘ 1 è¶Šå¥½ï¼‰\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>6.413075</td>\n",
       "      <td>0.257073</td>\n",
       "      <td>0.440403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>7.102914</td>\n",
       "      <td>0.332713</td>\n",
       "      <td>0.313539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CatBoostRegressor</td>\n",
       "      <td>7.263349</td>\n",
       "      <td>0.309797</td>\n",
       "      <td>0.282178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>7.394393</td>\n",
       "      <td>0.419739</td>\n",
       "      <td>0.256043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   model      RMSE       MAE        R2\n",
       "3  RandomForestRegressor  6.413075  0.257073  0.440403\n",
       "1           XGBRegressor  7.102914  0.332713  0.313539\n",
       "0      CatBoostRegressor  7.263349  0.309797  0.282178\n",
       "2          LGBMRegressor  7.394393  0.419739  0.256043"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æœ€ä½³å›å½’æ¨¡å‹ï¼šRandomForestRegressor | RMSE=6.413075, RÂ²=0.4404\n"
     ]
    }
   ],
   "source": [
    "# === Chunk 5: Regression model comparison ===\n",
    "def build_reg_model(backend: str):\n",
    "    if backend == \"catboost\" and CATBOOST_OK:\n",
    "        return (\"CatBoostRegressor\", CatBoostRegressor(random_state=CFG[\"random_state\"], **CFG[\"catboost\"]))\n",
    "    if backend == \"xgboost\" and XGBOOST_OK:\n",
    "        return (\"XGBRegressor\", XGBRegressor(random_state=CFG[\"random_state\"], **CFG[\"xgboost\"], objective=\"reg:squarederror\"))\n",
    "    if backend == \"lightgbm\" and LGBM_OK:\n",
    "        return (\"LGBMRegressor\", LGBMRegressor(random_state=CFG[\"random_state\"], **CFG[\"lightgbm\"]))\n",
    "    if backend == \"rf\":\n",
    "        return (\"RandomForestRegressor\", RandomForestRegressor(random_state=CFG[\"random_state\"], n_jobs=-1, **CFG[\"rf\"]))\n",
    "    return None\n",
    "\n",
    "reg_models = []\n",
    "for b in CFG[\"reg_backends\"]:\n",
    "    pair = build_reg_model(b)\n",
    "    if pair is not None: reg_models.append(pair)\n",
    "\n",
    "reg_results = []\n",
    "if target_reg is not None:\n",
    "    for name, mdl in reg_models:\n",
    "        pipe = Pipeline([(\"prep\", preprocessor), (\"mdl\", mdl)])\n",
    "        pipe.fit(Xtr, ytr)\n",
    "        pred = pipe.predict(Xte)\n",
    "        rmse = mean_squared_error(yte, pred, squared=False)\n",
    "        mae  = mean_absolute_error(yte, pred)\n",
    "        r2   = r2_score(yte, pred)\n",
    "\n",
    "        reg_results.append((name, rmse, mae, r2, pipe))\n",
    "        print(f\"ğŸ“Š æ¨¡å‹ï¼š{name}\")\n",
    "        print(f\"  RMSE: {rmse:.6f}ï¼ˆè¶Šå°è¶Šå¥½ï¼‰\")\n",
    "        print(f\"  MAE:  {mae:.6f}ï¼ˆè¶Šå°è¶Šå¥½ï¼‰\")\n",
    "        print(f\"  RÂ²:   {r2:.4f}ï¼ˆè¶Šæ¥è¿‘ 1 è¶Šå¥½ï¼‰\")\n",
    "        print(\"-\"*60)\n",
    "\n",
    "    reg_df = pd.DataFrame(reg_results, columns=[\"model\",\"RMSE\",\"MAE\",\"R2\",\"pipe\"]).sort_values(\"RMSE\")\n",
    "    display(reg_df[[\"model\",\"RMSE\",\"MAE\",\"R2\"]])\n",
    "    best_reg = reg_df.iloc[0]\n",
    "    print(f\"âœ… æœ€ä½³å›å½’æ¨¡å‹ï¼š{best_reg['model']} | RMSE={best_reg['RMSE']:.6f}, RÂ²={best_reg['R2']:.4f}\")\n",
    "else:\n",
    "    print(\"âš ï¸ æ— æ³•è¿›è¡Œå›å½’å¯¹æ¯”ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d51cab",
   "metadata": {},
   "source": [
    "## Chunk 6 â€” ç‰¹å¾è´¡çŒ®åº¦ï¼ˆSHAP æˆ–ç‰¹å¾é‡è¦æ€§ï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "601746fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PermutationExplainer explainer: 101it [00:27,  2.45it/s]                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP å…¨å±€é‡è¦æ€§ï¼ˆç»å¯¹å¹³å‡å€¼ Top10ï¼‰ï¼š\n",
      "  - Views: 0.335742\n",
      "  - Likes: 0.119901\n",
      "  - Shares: 0.010286\n",
      "  - Comments: 0.003807\n",
      "  - Weekday: 0.001109\n",
      "  - Platform_YouTube: 0.000845\n",
      "  - Month: 0.000578\n",
      "  - Region_Canada: 0.000368\n",
      "  - Content_Type_Reel: 0.000335\n",
      "  - Hashtag_#Music: 0.000264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Chunk 6: Explainability ===\n",
    "if target_reg is not None and 'best_reg' in locals():\n",
    "    pipe = best_reg[\"pipe\"]\n",
    "    try:\n",
    "        final_model = pipe.named_steps[\"mdl\"]\n",
    "        ohe = pipe.named_steps[\"prep\"].named_transformers_[\"cat\"]\n",
    "        feature_names = list(ohe.get_feature_names_out(cat_cols)) + num_cols\n",
    "\n",
    "        if SHAP_OK:\n",
    "            try:\n",
    "                sample_X = pipe.named_steps[\"prep\"].transform(Xte)[:300]\n",
    "                import shap\n",
    "                explainer = shap.Explainer(final_model.predict, sample_X)\n",
    "                shap_values = explainer(sample_X[:100])\n",
    "                print(\"SHAP å…¨å±€é‡è¦æ€§ï¼ˆç»å¯¹å¹³å‡å€¼ Top10ï¼‰ï¼š\")\n",
    "                imp = np.abs(shap_values.values).mean(axis=0)\n",
    "                top_idx = np.argsort(imp)[::-1][:10]\n",
    "                for i in top_idx:\n",
    "                    print(f\"  - {feature_names[i]}: {imp[i]:.6f}\")\n",
    "            except Exception as e:\n",
    "                print(\"âš ï¸ SHAP è®¡ç®—å¤±è´¥ï¼Œé€€åŒ–åˆ° feature_importances_ï¼š\", e)\n",
    "                raise\n",
    "        else:\n",
    "            raise RuntimeError(\"SHAP ä¸å¯ç”¨\")\n",
    "\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            importances = final_model.feature_importances_\n",
    "            order = np.argsort(importances)[::-1][:10]\n",
    "            print(\"ç‰¹å¾é‡è¦æ€§ Top10ï¼ˆåŸºäºæ ‘æ¨¡å‹ï¼‰ï¼š\")\n",
    "            for i in order:\n",
    "                print(f\"  - {feature_names[i]}: {importances[i]:.6f}\")\n",
    "        except Exception as e2:\n",
    "            print(\"âŒ æ— æ³•æå–ç‰¹å¾é‡è¦æ€§ï¼š\", e2)\n",
    "else:\n",
    "    print(\"âš ï¸ æ— æ³•è¿›è¡Œå¯è§£é‡Šæ€§åˆ†æã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088492be",
   "metadata": {},
   "source": [
    "## Chunk 7 â€” æ—¶é—´è¶‹åŠ¿åˆ†æï¼ˆProphet æˆ–å­£èŠ‚æ€§åˆ†è§£ï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5b9f75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ—¶é—´åºåˆ—ç‚¹æ•°ï¼š 105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:24:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "01:24:22 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æœªæ¥ 8 å‘¨é¢„æµ‹ï¼ˆå¹³å‡äº’åŠ¨ç‡ï¼‰ï¼š\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>yhat</th>\n",
       "      <th>yhat_lower</th>\n",
       "      <th>yhat_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>2024-01-07</td>\n",
       "      <td>0.735176</td>\n",
       "      <td>-0.022815</td>\n",
       "      <td>1.546408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>2024-01-14</td>\n",
       "      <td>0.738392</td>\n",
       "      <td>-0.087954</td>\n",
       "      <td>1.482836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>2024-01-21</td>\n",
       "      <td>0.741609</td>\n",
       "      <td>-0.042823</td>\n",
       "      <td>1.525645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>2024-01-28</td>\n",
       "      <td>0.744826</td>\n",
       "      <td>-0.059672</td>\n",
       "      <td>1.493412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>2024-02-04</td>\n",
       "      <td>0.748043</td>\n",
       "      <td>-0.000931</td>\n",
       "      <td>1.585894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>2024-02-11</td>\n",
       "      <td>0.751260</td>\n",
       "      <td>-0.039214</td>\n",
       "      <td>1.469342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>2024-02-18</td>\n",
       "      <td>0.754477</td>\n",
       "      <td>-0.025112</td>\n",
       "      <td>1.552130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>2024-02-25</td>\n",
       "      <td>0.757693</td>\n",
       "      <td>-0.004425</td>\n",
       "      <td>1.587108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ds      yhat  yhat_lower  yhat_upper\n",
       "105 2024-01-07  0.735176   -0.022815    1.546408\n",
       "106 2024-01-14  0.738392   -0.087954    1.482836\n",
       "107 2024-01-21  0.741609   -0.042823    1.525645\n",
       "108 2024-01-28  0.744826   -0.059672    1.493412\n",
       "109 2024-02-04  0.748043   -0.000931    1.585894\n",
       "110 2024-02-11  0.751260   -0.039214    1.469342\n",
       "111 2024-02-18  0.754477   -0.025112    1.552130\n",
       "112 2024-02-25  0.757693   -0.004425    1.587108"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆ†æï¼šè‹¥ yhat å‘ˆä¸Šå‡è¶‹åŠ¿ï¼Œå¯åŠ ç ï¼›è‹¥ä¸ç¡®å®šæ€§åŒºé—´å˜å®½ï¼Œéœ€è°¨æ…é¢„ç®—åˆ†é…ã€‚\n"
     ]
    }
   ],
   "source": [
    "# === Chunk 7: Time trend ===\n",
    "dtcol = CFG[\"date_col\"]\n",
    "if dtcol in df.columns and \"Engagement_Rate\" in df.columns:\n",
    "    ts = df.dropna(subset=[dtcol]).set_index(dtcol)[\"Engagement_Rate\"].resample(\"W\").mean().dropna()\n",
    "    print(\"æ—¶é—´åºåˆ—ç‚¹æ•°ï¼š\", len(ts))\n",
    "\n",
    "    if PROPHET_OK and len(ts)>20:\n",
    "        tmp = ts.reset_index().rename(columns={dtcol:\"ds\",\"Engagement_Rate\":\"y\"})\n",
    "        m = Prophet(seasonality_mode=\"additive\", weekly_seasonality=True, daily_seasonality=False)\n",
    "        m.fit(tmp)\n",
    "        future = m.make_future_dataframe(periods=8, freq=\"W\")\n",
    "        fcst = m.predict(future).tail(8)[[\"ds\",\"yhat\",\"yhat_lower\",\"yhat_upper\"]]\n",
    "        print(\"æœªæ¥ 8 å‘¨é¢„æµ‹ï¼ˆå¹³å‡äº’åŠ¨ç‡ï¼‰ï¼š\")\n",
    "        display(fcst)\n",
    "        print(\"åˆ†æï¼šè‹¥ yhat å‘ˆä¸Šå‡è¶‹åŠ¿ï¼Œå¯åŠ ç ï¼›è‹¥ä¸ç¡®å®šæ€§åŒºé—´å˜å®½ï¼Œéœ€è°¨æ…é¢„ç®—åˆ†é…ã€‚\")\n",
    "    elif SM_SEASONAL_OK and len(ts)>20:\n",
    "        from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "        res = seasonal_decompose(ts, model=\"additive\", period=4)\n",
    "        print(\"å­£èŠ‚æ€§/è¶‹åŠ¿åˆ†è§£å®Œæˆã€‚è¶‹åŠ¿å¤´éƒ¨ 5ï¼š\")\n",
    "        display(res.trend.dropna().head())\n",
    "        print(\"åˆ†æï¼šè§‚å¯Ÿ trend ä¸ seasonalï¼Œå¯æŒ‡å¯¼â€œå‘¨å†…/å­£åº¦â€å‘å¸–èŠ‚å¥ã€‚\")\n",
    "    else:\n",
    "        print(\"âš ï¸ æ•°æ®ç‚¹ä¸è¶³æˆ–ä¾èµ–ä¸å¯ç”¨ï¼Œè·³è¿‡æ—¶é—´åºåˆ—é¢„æµ‹ã€‚\")\n",
    "else:\n",
    "    print(\"âš ï¸ ç¼ºå°‘æ—¥æœŸåˆ—æˆ–äº’åŠ¨ç‡ï¼Œè·³è¿‡æ—¶é—´è¶‹åŠ¿åˆ†æã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2afdfe9",
   "metadata": {},
   "source": [
    "## Chunk 8 â€” çˆ†æ¬¾åˆ†ç±»ï¼ˆå¹³å°å†… Top p%ï¼Œp æ¥æºäº CFGï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c38d5401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ˆ åˆ†ç±»æ¨¡å‹ï¼šCatBoostClassifier | weighted F1=0.9873\n",
      "ğŸ“ˆ åˆ†ç±»æ¨¡å‹ï¼šXGBClassifier | weighted F1=0.9865\n",
      "ğŸ“ˆ åˆ†ç±»æ¨¡å‹ï¼šLGBMClassifier | weighted F1=0.9873\n",
      "ğŸ“ˆ åˆ†ç±»æ¨¡å‹ï¼šRandomForestClassifier | weighted F1=0.9803\n",
      "âœ… æœ€ä½³åˆ†ç±»æ¨¡å‹ï¼šLGBMClassifier | weighted F1=0.9873\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9955    0.9902    0.9929      1125\n",
      "           1     0.9160    0.9600    0.9375       125\n",
      "\n",
      "    accuracy                         0.9872      1250\n",
      "   macro avg     0.9558    0.9751    0.9652      1250\n",
      "weighted avg     0.9876    0.9872    0.9873      1250\n",
      "\n",
      "åˆ†æï¼šF1 è¶Šé«˜è¡¨ç¤ºçˆ†æ¬¾/éçˆ†æ¬¾æ•´ä½“è¡¨ç°æ›´å¥½ï¼›ç”¨ OHE åˆå¹¶ç¨€æœ‰ + æ–¹å·®è¿‡æ»¤å¯å‡å°‘â€œæ— æ­£å¢ç›Šâ€æƒ…å†µã€‚\n",
      "æç¤ºï¼šè‹¥ä¾æ—§å‡ºç°è¯¥æç¤ºä¸”ä½ ä¸éœ€è¦ LGBMï¼Œå¯åœ¨ CFG['clf_backends'] é‡Œç§»é™¤ 'lightgbm'ï¼Œä¼˜å…ˆç”¨ CatBoost/XGBoostã€‚\n"
     ]
    }
   ],
   "source": [
    "# === Chunk 8: Virality classification â€” å¼ºåŒ– & é™é»˜ç‰ˆ ===\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from collections import Counter\n",
    "from packaging import version\n",
    "import sklearn\n",
    "\n",
    "VIRAL_P = CFG[\"viral_percentile\"]\n",
    "\n",
    "def build_viral_labels(_df, metric=\"Engagement_Rate\", platform_col=\"Platform\", p=VIRAL_P):\n",
    "    df2 = _df.copy()\n",
    "    if metric not in df2.columns:\n",
    "        return None\n",
    "    df2[metric] = pd.to_numeric(df2[metric], errors=\"coerce\")\n",
    "    if platform_col in df2.columns:\n",
    "        def _mark(g):\n",
    "            thr = g[metric].quantile(p)\n",
    "            g[\"viral\"] = (g[metric] >= thr).astype(int)\n",
    "            return g\n",
    "        return df2.groupby(platform_col, group_keys=False).apply(_mark)\n",
    "    else:\n",
    "        thr = df2[metric].quantile(p)\n",
    "        df2[\"viral\"] = (df2[metric] >= thr).astype(int)\n",
    "        return df2\n",
    "\n",
    "if \"Engagement_Rate\" in df.columns:\n",
    "    df_v = build_viral_labels(df, metric=\"Engagement_Rate\", platform_col=\"Platform\", p=VIRAL_P)\n",
    "    if df_v is not None and df_v[\"viral\"].nunique() == 2:\n",
    "        # â‘  åˆå¹¶ä½é¢‘ç±»åˆ«ï¼ˆä¸æ­¢ Hashtagï¼‰\n",
    "        def collapse_rare(series, k=20):\n",
    "            vc = series.astype(str).value_counts()\n",
    "            keep = set(vc.head(k).index)\n",
    "            return series.astype(str).where(series.astype(str).isin(keep), \"_other_\")\n",
    "\n",
    "        cat_cols_c = [c for c in [\"Platform\",\"Content_Type\",\"Region\",\"Hashtag\"] if c in df.columns]\n",
    "        for c in cat_cols_c:\n",
    "            df_v[c] = collapse_rare(df_v[c], k=20)\n",
    "\n",
    "        num_cols_c = [c for c in [\"Views\",\"Likes\",\"Shares\",\"Comments\",\"Hour\",\"Weekday\",\"Month\"] if c in df.columns]\n",
    "\n",
    "        X_clf = df_v[cat_cols_c + num_cols_c].copy()\n",
    "        y_clf = df_v[\"viral\"].astype(int).values\n",
    "\n",
    "        Xtr_c, Xte_c, ytr_c, yte_c = train_test_split(\n",
    "            X_clf, y_clf,\n",
    "            test_size=CFG[\"test_size\"],\n",
    "            random_state=CFG[\"random_state\"],\n",
    "            stratify=y_clf\n",
    "        )\n",
    "\n",
    "        # ç±»åˆ«ä¸å¹³è¡¡æƒé‡ï¼ˆç»™ LGBM / XGB ç”¨ï¼‰\n",
    "        cnt = Counter(ytr_c)\n",
    "        pos = cnt.get(1, 0)\n",
    "        neg = cnt.get(0, 0)\n",
    "        scale_pos_weight = float(neg) / max(1.0, float(pos)) if (pos > 0) else 1.0\n",
    "\n",
    "        # â‘¡ é¢„å¤„ç†ï¼šOHE åˆå¹¶ç¨€æœ‰ç±»åˆ« + æ–¹å·®è¿‡æ»¤è¿‘å¸¸é‡åˆ—\n",
    "        supports_minfreq = version.parse(sklearn.__version__) >= version.parse(\"1.1\")\n",
    "        if supports_minfreq:\n",
    "            ohe = OneHotEncoder(\n",
    "                handle_unknown=\"infrequent_if_exist\",\n",
    "                min_frequency=0.01,   # 1% ä»¥ä¸‹å¹¶å…¥â€œinfrequentâ€\n",
    "                sparse=True\n",
    "            )\n",
    "        else:\n",
    "            ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "\n",
    "        pre_clf = ColumnTransformer([\n",
    "            (\"cat\", ohe, cat_cols_c),\n",
    "            (\"num\", StandardScaler(with_mean=False), num_cols_c)  # ç¨€ç–å…¼å®¹\n",
    "        ])\n",
    "        vt = VarianceThreshold(threshold=1e-5)  # æ¯” 0 æ›´ä¸¥æ ¼ï¼Œå»è¿‘å¸¸é‡åˆ—\n",
    "\n",
    "        def build_clf_model(backend: str):\n",
    "            if backend == \"catboost\" and CATBOOST_OK:\n",
    "                params = CFG[\"catboost\"].copy()\n",
    "                return (\"CatBoostClassifier\",\n",
    "                        CatBoostClassifier(random_state=CFG[\"random_state\"],\n",
    "                                           loss_function=\"Logloss\",\n",
    "                                           **params))\n",
    "            if backend == \"xgboost\" and XGBOOST_OK:\n",
    "                p = CFG[\"xgboost\"].copy()\n",
    "                p.update(dict(objective=\"binary:logistic\",\n",
    "                              eval_metric=\"logloss\",\n",
    "                              scale_pos_weight=scale_pos_weight,\n",
    "                              tree_method=\"hist\",\n",
    "                              n_jobs=-1))\n",
    "                return (\"XGBClassifier\", XGBClassifier(random_state=CFG[\"random_state\"], **p))\n",
    "            if backend == \"lightgbm\" and LGBM_OK:\n",
    "                # â€”â€” æ›´ä¿å®ˆ + é™é»˜ â€”â€” \n",
    "                p = dict(\n",
    "                    n_estimators=CFG[\"lightgbm\"].get(\"n_estimators\", 700),\n",
    "                    learning_rate=CFG[\"lightgbm\"].get(\"learning_rate\", 0.05),\n",
    "                    num_leaves=15,                 # â†“å°ä¸€äº›æ›´ç¨³\n",
    "                    max_depth=6,                   # é™æ·±é¿å…ç›²ç›®ä¸‹æ¢\n",
    "                    min_child_samples=5,           # æ”¾å®½ï¼Œæé«˜å¯åˆ†å¶\n",
    "                    min_split_gain=0.0,\n",
    "                    min_child_weight=1e-3,\n",
    "                    feature_fraction=1.0,          # ä¸æŠ½åˆ—ï¼Œé¿å…æ›´å°‘å¯åˆ†åˆ—\n",
    "                    bagging_fraction=1.0,          # ä¸æŠ½è¡Œ\n",
    "                    bagging_freq=0,\n",
    "                    objective=\"binary\",\n",
    "                    scale_pos_weight=scale_pos_weight,\n",
    "                    reg_alpha=0.0,\n",
    "                    reg_lambda=0.0,\n",
    "                    n_jobs=-1,\n",
    "                    random_state=CFG[\"random_state\"],\n",
    "                    verbosity=-1                   # â† é™é»˜ LightGBM æ—¥å¿—ï¼ˆå«ä½ çœ‹åˆ°çš„ warningï¼‰\n",
    "                )\n",
    "                return (\"LGBMClassifier\", LGBMClassifier(**p))\n",
    "            if backend == \"rf\":\n",
    "                return (\"RandomForestClassifier\",\n",
    "                        RandomForestClassifier(random_state=CFG[\"random_state\"],\n",
    "                                               n_jobs=-1,\n",
    "                                               **CFG[\"rf\"]))\n",
    "            return None\n",
    "\n",
    "        clf_candidates = []\n",
    "        for b in CFG[\"clf_backends\"]:\n",
    "            pair = build_clf_model(b)\n",
    "            if pair is not None:\n",
    "                clf_candidates.append(pair)\n",
    "\n",
    "        best_f1 = -1.0\n",
    "        best_clf = None\n",
    "        for name, clf in clf_candidates:\n",
    "            pipe = Pipeline([\n",
    "                (\"prep\", pre_clf),\n",
    "                (\"vt\", vt),          # è¿‡æ»¤è¿‘å¸¸é‡åˆ—\n",
    "                (\"clf\", clf)\n",
    "            ])\n",
    "            pipe.fit(Xtr_c, ytr_c)\n",
    "            ypred = pipe.predict(Xte_c)\n",
    "            report = classification_report(yte_c, ypred, digits=4, output_dict=True)\n",
    "            f1 = report[\"weighted avg\"][\"f1-score\"]\n",
    "            print(f\"ğŸ“ˆ åˆ†ç±»æ¨¡å‹ï¼š{name} | weighted F1={f1:.4f}\")\n",
    "            if f1 > best_f1:\n",
    "                best_clf = (name, pipe, f1)\n",
    "                best_f1 = f1\n",
    "\n",
    "        print(f\"âœ… æœ€ä½³åˆ†ç±»æ¨¡å‹ï¼š{best_clf[0]} | weighted F1={best_clf[2]:.4f}\")\n",
    "        ypred_best = best_clf[1].predict(Xte_c)\n",
    "        print(classification_report(yte_c, ypred_best, digits=4))\n",
    "        print(\"åˆ†æï¼šF1 è¶Šé«˜è¡¨ç¤ºçˆ†æ¬¾/éçˆ†æ¬¾æ•´ä½“è¡¨ç°æ›´å¥½ï¼›ç”¨ OHE åˆå¹¶ç¨€æœ‰ + æ–¹å·®è¿‡æ»¤å¯å‡å°‘â€œæ— æ­£å¢ç›Šâ€æƒ…å†µã€‚\")\n",
    "        print(\"æç¤ºï¼šè‹¥ä¾æ—§å‡ºç°è¯¥æç¤ºä¸”ä½ ä¸éœ€è¦ LGBMï¼Œå¯åœ¨ CFG['clf_backends'] é‡Œç§»é™¤ 'lightgbm'ï¼Œä¼˜å…ˆç”¨ CatBoost/XGBoostã€‚\")\n",
    "    else:\n",
    "        print(\"âš ï¸ æ— æ³•æ„é€ äºŒåˆ†ç±»æ ‡ç­¾ï¼ˆå¯èƒ½æ ·æœ¬ä¸è¶³æˆ–ç›®æ ‡åˆ—ç¼ºå¤±ï¼‰ã€‚\")\n",
    "else:\n",
    "    print(\"âš ï¸ æ— æ³•è¿›è¡Œçˆ†æ¬¾åˆ†ç±»ï¼ˆç¼ºå°‘ Engagement_Rateï¼‰ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d920b1",
   "metadata": {},
   "source": [
    "## Chunk 9 â€” å—ä¼—/å†…å®¹èšç±»ï¼ˆKMeansï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57b36f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=3 -> è½®å»“ç³»æ•°=0.0768\n",
      "k=4 -> è½®å»“ç³»æ•°=0.0690\n",
      "k=5 -> è½®å»“ç³»æ•°=0.0671\n",
      "k=6 -> è½®å»“ç³»æ•°=0.0667\n",
      "âœ… é€‰å®šç°‡æ•°ï¼šk=3 | è½®å»“ç³»æ•°=0.0768\n",
      "å„ç°‡è§„æ¨¡ï¼š\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count\n",
       "cluster       \n",
       "0         2515\n",
       "1         2484\n",
       "2            1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å„ç°‡æ•°å€¼ç‰¹å¾å‡å€¼ï¼š\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Views</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Shares</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>Month</th>\n",
       "      <th>Engagement_Rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.577800e+06</td>\n",
       "      <td>248669.0584</td>\n",
       "      <td>54579.0036</td>\n",
       "      <td>24933.1769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.6998</td>\n",
       "      <td>6.6771</td>\n",
       "      <td>0.4786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.410291e+06</td>\n",
       "      <td>254288.8961</td>\n",
       "      <td>46429.1067</td>\n",
       "      <td>24838.5298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2834</td>\n",
       "      <td>6.4690</td>\n",
       "      <td>0.5445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.266000e+03</td>\n",
       "      <td>318849.0000</td>\n",
       "      <td>1715.0000</td>\n",
       "      <td>36121.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>281.7417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Views        Likes      Shares    Comments  Hour  Weekday  \\\n",
       "cluster                                                                     \n",
       "0        2.577800e+06  248669.0584  54579.0036  24933.1769   0.0   4.6998   \n",
       "1        2.410291e+06  254288.8961  46429.1067  24838.5298   0.0   1.2834   \n",
       "2        1.266000e+03  318849.0000   1715.0000  36121.0000   0.0   6.0000   \n",
       "\n",
       "          Month  Engagement_Rate  \n",
       "cluster                           \n",
       "0        6.6771           0.4786  \n",
       "1        6.4690           0.5445  \n",
       "2        9.0000         281.7417  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆ†æï¼šä¸åŒç°‡çš„ç‰¹å¾å‡å€¼å·®å¼‚ï¼Œèƒ½è¾…åŠ©åˆ¶å®šå·®å¼‚åŒ–å†…å®¹/æŠ•æ”¾ç­–ç•¥ã€‚\n"
     ]
    }
   ],
   "source": [
    "# === Chunk 9: KMeans clustering ===\n",
    "cat_cols_k = [c for c in [\"Platform\",\"Content_Type\",\"Region\",\"Hashtag\"] if c in df.columns]\n",
    "num_cols_k = [c for c in [\"Views\",\"Likes\",\"Shares\",\"Comments\",\"Hour\",\"Weekday\",\"Month\",\"Engagement_Rate\"] if c in df.columns]\n",
    "\n",
    "if len(num_cols_k)+len(cat_cols_k) > 0:\n",
    "    pre_all = ColumnTransformer([(\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols_k),\n",
    "                                 (\"num\", StandardScaler(), num_cols_k)])\n",
    "    X_all = pre_all.fit_transform(df[cat_cols_k + num_cols_k].fillna(0))\n",
    "\n",
    "    best_k, best_score = None, -1\n",
    "    for k in [3,4,5,6]:\n",
    "        try:\n",
    "            km = KMeans(n_clusters=k, random_state=CFG[\"random_state\"], n_init=10)\n",
    "            labels = km.fit_predict(X_all)\n",
    "            score = silhouette_score(X_all, labels)\n",
    "            print(f\"k={k} -> è½®å»“ç³»æ•°={score:.4f}\")\n",
    "            if score > best_score:\n",
    "                best_k, best_score, best_km = k, score, km\n",
    "        except Exception as e:\n",
    "            print(f\"k={k} è®¡ç®—å¤±è´¥ï¼š\", e)\n",
    "\n",
    "    if best_k is not None:\n",
    "        labels = best_km.predict(X_all)\n",
    "        df[\"cluster\"] = labels\n",
    "        print(f\"âœ… é€‰å®šç°‡æ•°ï¼šk={best_k} | è½®å»“ç³»æ•°={best_score:.4f}\")\n",
    "        print(\"å„ç°‡è§„æ¨¡ï¼š\")\n",
    "        display(df[\"cluster\"].value_counts().rename(\"count\").to_frame())\n",
    "\n",
    "        if num_cols_k:\n",
    "            print(\"å„ç°‡æ•°å€¼ç‰¹å¾å‡å€¼ï¼š\")\n",
    "            display(df.groupby(\"cluster\")[num_cols_k].mean().round(4))\n",
    "        print(\"åˆ†æï¼šä¸åŒç°‡çš„ç‰¹å¾å‡å€¼å·®å¼‚ï¼Œèƒ½è¾…åŠ©åˆ¶å®šå·®å¼‚åŒ–å†…å®¹/æŠ•æ”¾ç­–ç•¥ã€‚\")\n",
    "    else:\n",
    "        print(\"âš ï¸ èšç±»æœªæˆåŠŸã€‚\")\n",
    "else:\n",
    "    print(\"âš ï¸ æ— è¶³å¤Ÿç‰¹å¾ç”¨äºèšç±»ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65834c39",
   "metadata": {},
   "source": [
    "## Chunk 10 â€” å¹¿å‘Š ROI å»ºæ¨¡ï¼ˆè‹¥å­—æ®µå­˜åœ¨ï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ca7faa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ æœªæ£€æµ‹åˆ°å¹¿å‘Šå­—æ®µï¼ˆAdSpend/CTR/ConversionRate ç­‰ï¼‰ã€‚\n",
      "å»ºè®®ï¼šå¦‚ç¼ºä¹å¹¿å‘Šæ•°æ®ï¼Œå¯ç”¨ Engagement_Rate ä½œä¸ºå†…å®¹æ•ˆèƒ½çš„æ›¿ä»£æŒ‡æ ‡ï¼Œ\n",
      "å¹¶åœ¨åª’ä½“å¹³å°æŠ¥è¡¨ä¸­ç»“åˆæ›å…‰/ç‚¹å‡»/è½¬åŒ–æ¥è¡¥é½ ROI è§†è§’ã€‚\n"
     ]
    }
   ],
   "source": [
    "# === Chunk 10: Ads ROI modeling (optional) ===\n",
    "ad_cols = [c for c in [\"AdSpend\",\"ClickThroughRate\",\"ConversionRate\",\"WebsiteVisits\",\"PagesPerVisit\",\"TimeOnSite\",\"EmailOpens\",\"EmailClicks\"] if c in df.columns]\n",
    "\n",
    "if \"AdSpend\" in ad_cols and \"ConversionRate\" in ad_cols:\n",
    "    df_ads = df.copy()\n",
    "    df_ads[\"eCPA\"] = (df_ads[\"AdSpend\"] / (df_ads[\"ConversionRate\"].replace(0, np.nan))).replace([np.inf, -np.inf], np.nan)\n",
    "    df_ads = df_ads.dropna(subset=[\"eCPA\"])\n",
    "\n",
    "    cat_ad = [c for c in [\"Platform\",\"Content_Type\",\"Region\"] if c in df_ads.columns]\n",
    "    num_ad = [c for c in [\"Views\",\"Likes\",\"Shares\",\"Comments\",\"Hour\",\"Weekday\",\"Month\",\"AdSpend\",\"ClickThroughRate\",\"ConversionRate\"] if c in df_ads.columns]\n",
    "\n",
    "    X_ad = df_ads[cat_ad + num_ad].copy()\n",
    "    y_ad = df_ads[\"eCPA\"].astype(float).values\n",
    "\n",
    "    pre_ad = ColumnTransformer([(\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_ad),\n",
    "                                (\"num\", StandardScaler(), num_ad)])\n",
    "\n",
    "    # ç”¨ç»Ÿä¸€é…ç½®é€‰æ‹©æ¨¡å‹\n",
    "    def build_reg_model(backend: str):\n",
    "        if backend == \"catboost\" and CATBOOST_OK:\n",
    "            return (\"CatBoostRegressor\", CatBoostRegressor(random_state=CFG[\"random_state\"], **CFG[\"catboost\"]))\n",
    "        if backend == \"xgboost\" and XGBOOST_OK:\n",
    "            return (\"XGBRegressor\", XGBRegressor(random_state=CFG[\"random_state\"], **CFG[\"xgboost\"], objective=\"reg:squarederror\"))\n",
    "        if backend == \"lightgbm\" and LGBM_OK:\n",
    "            return (\"LGBMRegressor\", LGBMRegressor(random_state=CFG[\"random_state\"], **CFG[\"lightgbm\"]))\n",
    "        if backend == \"rf\":\n",
    "            return (\"RandomForestRegressor\", RandomForestRegressor(random_state=CFG[\"random_state\"], n_jobs=-1, **CFG[\"rf\"]))\n",
    "        return None\n",
    "\n",
    "    pair = None\n",
    "    for b in CFG[\"reg_backends\"]:\n",
    "        pair = build_reg_model(b)\n",
    "        if pair is not None:\n",
    "            break\n",
    "\n",
    "    mdl_name, mdl = pair\n",
    "    pipe_ad = Pipeline([(\"prep\", pre_ad), (\"mdl\", mdl)])\n",
    "    Xtr_a, Xte_a, ytr_a, yte_a = train_test_split(X_ad, y_ad, test_size=CFG[\"test_size\"], random_state=CFG[\"random_state\"])\n",
    "    pipe_ad.fit(Xtr_a, ytr_a)\n",
    "    pred_a = pipe_ad.predict(Xte_a)\n",
    "\n",
    "    rmse = mean_squared_error(yte_a, pred_a, squared=False)\n",
    "    mae  = mean_absolute_error(yte_a, pred_a)\n",
    "    r2   = r2_score(yte_a, pred_a)\n",
    "\n",
    "    print(f\"ğŸ“Š ROI æ¨¡å‹ï¼ˆ{mdl_name}ï¼ŒeCPA é¢„æµ‹ï¼‰\")\n",
    "    print(f\"  RMSE={rmse:.4f}  MAE={mae:.4f}  RÂ²={r2:.4f}\")\n",
    "    print(\"åˆ†æï¼šRMSE/MAE è¶Šå°è¶Šå¥½ï¼ŒRÂ² è¶Šæ¥è¿‘ 1 è¶Šå¥½ï¼›å¯è¾…åŠ©é¢„ç®—åˆ†é…ä¸æ¸ é“ä¼˜å…ˆçº§ã€‚\")\n",
    "else:\n",
    "    print(\"âš ï¸ æœªæ£€æµ‹åˆ°å¹¿å‘Šå­—æ®µï¼ˆAdSpend/CTR/ConversionRate ç­‰ï¼‰ã€‚\")\n",
    "    print(\"å»ºè®®ï¼šå¦‚ç¼ºä¹å¹¿å‘Šæ•°æ®ï¼Œå¯ç”¨ Engagement_Rate ä½œä¸ºå†…å®¹æ•ˆèƒ½çš„æ›¿ä»£æŒ‡æ ‡ï¼Œ\")\n",
    "    print(\"å¹¶åœ¨åª’ä½“å¹³å°æŠ¥è¡¨ä¸­ç»“åˆæ›å…‰/ç‚¹å‡»/è½¬åŒ–æ¥è¡¥é½ ROI è§†è§’ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dcf156",
   "metadata": {},
   "source": [
    "## Chunk 11 â€” æ•æ„Ÿæ€§åˆ†æï¼ˆWhat-if æ¨¡æ‹Ÿï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3d2c531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•æ„Ÿæ€§æ’åï¼ˆæŒ‰å¹³å‡å½±å“ç»å¯¹å€¼ï¼‰ï¼š\n",
      "  - Views: 7.600751\n",
      "  - Weekday: 0.014317\n",
      "  - Hour: 0.000000\n",
      "\n",
      "ç¤ºä¾‹ â€”â€” æœ€æ•æ„Ÿç‰¹å¾çš„å˜åŒ–æ›²çº¿ï¼š Views\n",
      "  Views=1266  ->  é¢„æµ‹å˜åŒ–(ç›¸å¯¹åŸºçº¿å‡å€¼)=57.983478\n",
      "  Views=715289  ->  é¢„æµ‹å˜åŒ–(ç›¸å¯¹åŸºçº¿å‡å€¼)=-0.135649\n",
      "  Views=1429312  ->  é¢„æµ‹å˜åŒ–(ç›¸å¯¹åŸºçº¿å‡å€¼)=-0.349206\n",
      "  Views=2143336  ->  é¢„æµ‹å˜åŒ–(ç›¸å¯¹åŸºçº¿å‡å€¼)=-0.418870\n",
      "  Views=2857359  ->  é¢„æµ‹å˜åŒ–(ç›¸å¯¹åŸºçº¿å‡å€¼)=-0.455016\n",
      "  Views=3571383  ->  é¢„æµ‹å˜åŒ–(ç›¸å¯¹åŸºçº¿å‡å€¼)=-0.475819\n",
      "  Views=4285406  ->  é¢„æµ‹å˜åŒ–(ç›¸å¯¹åŸºçº¿å‡å€¼)=-0.490866\n",
      "  Views=4999430  ->  é¢„æµ‹å˜åŒ–(ç›¸å¯¹åŸºçº¿å‡å€¼)=-0.497104\n"
     ]
    }
   ],
   "source": [
    "# === Chunk 11: Sensitivity analysis ===\n",
    "def what_if_change(pipe, df_src, feature, values, cat_cols, num_cols, target_name=\"Engagement_Rate\"):\n",
    "    df_tmp = df_src[cat_cols + num_cols].copy()\n",
    "    outs = []\n",
    "    if feature not in df_tmp.columns:\n",
    "        return None\n",
    "    baseline = pipe.predict(df_tmp.head(200))\n",
    "    for v in values:\n",
    "        df_changed = df_tmp.copy()\n",
    "        df_changed[feature] = v\n",
    "        pred = pipe.predict(df_changed.head(200))\n",
    "        outs.append((v, float(np.mean(pred - baseline))))\n",
    "    return outs\n",
    "\n",
    "if target_reg is not None and 'best_reg' in locals():\n",
    "    pipe = best_reg[\"pipe\"]\n",
    "    candidates = [c for c in [\"Hour\",\"Weekday\",\"Views\"] if c in df.columns]\n",
    "    sens_all = []\n",
    "    for f in candidates:\n",
    "        vals = sorted(df[f].dropna().unique())\n",
    "        import numpy as np\n",
    "        if len(vals)>10:\n",
    "            vals = np.linspace(np.nanmin(vals), np.nanmax(vals), 8).astype(int if df[f].dtype.kind in \"iu\" else float)\n",
    "        diffs = what_if_change(pipe, df, f, vals, cat_cols, num_cols, target_name=\"Engagement_Rate\")\n",
    "        if diffs:\n",
    "            mean_abs = np.mean([abs(x[1]) for x in diffs])\n",
    "            sens_all.append((f, mean_abs, diffs))\n",
    "\n",
    "    if sens_all:\n",
    "        sens_all.sort(key=lambda x: x[1], reverse=True)\n",
    "        print(\"æ•æ„Ÿæ€§æ’åï¼ˆæŒ‰å¹³å‡å½±å“ç»å¯¹å€¼ï¼‰ï¼š\")\n",
    "        for f, mag, _ in sens_all:\n",
    "            print(f\"  - {f}: {mag:.6f}\")\n",
    "        top = sens_all[0]\n",
    "        print(\"\\nç¤ºä¾‹ â€”â€” æœ€æ•æ„Ÿç‰¹å¾çš„å˜åŒ–æ›²çº¿ï¼š\", top[0])\n",
    "        for v, d in top[2]:\n",
    "            print(f\"  {top[0]}={v}  ->  é¢„æµ‹å˜åŒ–(ç›¸å¯¹åŸºçº¿å‡å€¼)={d:.6f}\")\n",
    "    else:\n",
    "        print(\"âš ï¸ æ— æ³•è®¡ç®—æ•æ„Ÿæ€§ï¼ˆå¯èƒ½æ ·æœ¬ä¸è¶³ï¼‰ã€‚\")\n",
    "else:\n",
    "    print(\"âš ï¸ æ— æ³•è¿›è¡Œæ•æ„Ÿæ€§åˆ†æã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92cc29c",
   "metadata": {},
   "source": [
    "## Chunk 12 â€” æ®‹å·®ä¸è¯¯å·®ç»“æ„\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27f7eb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ®‹å·®å‡å€¼ï¼š 0.049727079833028785\n",
      "æ®‹å·®æ ‡å‡†å·®ï¼š 3.233105426521446\n",
      "è¿‡å¤§å¼‚å¸¸ï¼ˆ>|3Ïƒ|ï¼‰å æ¯”ï¼š 0.001\n",
      "åˆ†æï¼šæ®‹å·®å‡å€¼æ¥è¿‘ 0 ä¸”æç«¯æ®‹å·®å æ¯”ä½ï¼Œè¯´æ˜æ¨¡å‹æ€»ä½“æ— ç³»ç»Ÿæ€§åå·®ã€‚\n"
     ]
    }
   ],
   "source": [
    "# === Chunk 12: Residuals ===\n",
    "if target_reg is not None and 'best_reg' in locals():\n",
    "    pipe = best_reg[\"pipe\"]\n",
    "    pred_full = pipe.predict(df[cat_cols + num_cols].copy())\n",
    "    residuals = df[\"Engagement_Rate\"].values - pred_full\n",
    "    import numpy as np\n",
    "    print(\"æ®‹å·®å‡å€¼ï¼š\", float(np.mean(residuals)))\n",
    "    print(\"æ®‹å·®æ ‡å‡†å·®ï¼š\", float(np.std(residuals)))\n",
    "    print(\"è¿‡å¤§å¼‚å¸¸ï¼ˆ>|3Ïƒ|ï¼‰å æ¯”ï¼š\", float(np.mean(np.abs(residuals) > (3*np.std(residuals)))))\n",
    "    print(\"åˆ†æï¼šæ®‹å·®å‡å€¼æ¥è¿‘ 0 ä¸”æç«¯æ®‹å·®å æ¯”ä½ï¼Œè¯´æ˜æ¨¡å‹æ€»ä½“æ— ç³»ç»Ÿæ€§åå·®ã€‚\")\n",
    "else:\n",
    "    print(\"âš ï¸ æ®‹å·®åˆ†æä¸å¯ç”¨ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb86a3fb",
   "metadata": {},
   "source": [
    "## Chunk 13 â€” è¡Œä¸šåŒ–æ€»ç»“ä¸ç­–ç•¥å»ºè®®ï¼ˆA+B+Cï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "405b2ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€”â€” æ€»ç»“æŠ¥å‘Šï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰ â€”â€”\n",
      "[å†…å®¹é¢„æµ‹] æœ€ä½³å›å½’æ¨¡å‹ï¼šRandomForestRegressorï¼ŒRMSE=6.413075ï¼ŒRÂ²=0.4404ã€‚\n",
      "è§£è¯»ï¼šè¯¥æ¨¡å‹å¯¹äº’åŠ¨ç‡çš„è§£é‡ŠåŠ›è¾ƒå¼ºï¼Œå¯ç”¨äºå†…å®¹é€‰é¢˜ä¸æŠ•æ”¾æ’æœŸçš„é‡åŒ–è¯„ä¼°ã€‚\n",
      "[å—ä¼—/å†…å®¹èšç±»] æœ€ä¼˜ç°‡æ•°ï¼š3ï¼Œç°‡è§„æ¨¡ï¼š[2515, 2484, 1]\n",
      "è§£è¯»ï¼šç»“åˆå„ç°‡çš„æ•°å€¼å‡å€¼å·®å¼‚ï¼Œå¯ä¸ºä¸åŒäººç¾¤è¾“å‡ºå·®å¼‚åŒ–å†…å®¹ä¸æŠ•æ”¾æ—¶æ®µå»ºè®®ã€‚\n",
      "[å¹¿å‘ŠROI] æ•°æ®ç¼ºå°‘æŠ•æ”¾å­—æ®µï¼Œå»ºè®®åœ¨åª’ä½“ç«¯è¡¥å……æ›å…‰ã€ç‚¹å‡»ã€è½¬åŒ–æ•°æ®åå†å»ºæ¨¡ã€‚\n",
      "â€”â€” ç­–ç•¥å»ºè®® â€”â€”\n",
      "1) å¹³å°å·®å¼‚ï¼šé«˜åŸºçº¿å¹³å°å¯è¿½æ±‚è§„æ¨¡ï¼Œä½åŸºçº¿å¹³å°è¿½æ±‚ROIï¼›ç»“åˆæ—¶é—´æ®µï¼ˆHour/Weekdayï¼‰çš„æ•æ„Ÿæ€§ç»“æœè¿›è¡Œæ’æœŸä¼˜åŒ–ã€‚\n",
      "2) å†…å®¹ç±»å‹ï¼šä¼˜å…ˆåˆ¶ä½œå¯¹äº’åŠ¨ç‡è´¡çŒ®åº¦é«˜çš„å†…å®¹ç±»å‹ï¼›å¿…è¦æ—¶åš A/B æµ‹è¯•éªŒè¯ã€‚\n",
      "3) å¹¿å‘ŠæŠ•æ”¾ï¼šè‹¥å¯ç”¨ï¼Œä½¿ç”¨ ROI æ¨¡å‹å¯¹ eCPA è¿›è¡Œé¢„æµ‹åˆ†å±‚ï¼Œé¢„ç®—å‘ä½ eCPAã€ç¨³å®šäººç¾¤å€¾æ–œã€‚\n",
      "4) å—ä¼—åˆ†ç¾¤ï¼šå¯¹æ¯ä¸ªç°‡åˆ¶å®šä¸åŒåˆ›æ„ä¸æŠ•æ”¾æ—¶æ®µï¼›å°†èšç±»æ ‡ç­¾å›å¡«è‡³BIç³»ç»ŸæŒç»­è·Ÿè¸ªè¡¨ç°ã€‚\n"
     ]
    }
   ],
   "source": [
    "# === Chunk 13: Summary ===\n",
    "print(\"â€”â€” æ€»ç»“æŠ¥å‘Šï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰ â€”â€”\")\n",
    "if 'best_reg' in locals():\n",
    "    print(f\"[å†…å®¹é¢„æµ‹] æœ€ä½³å›å½’æ¨¡å‹ï¼š{best_reg['model']}ï¼ŒRMSE={best_reg['RMSE']:.6f}ï¼ŒRÂ²={best_reg['R2']:.4f}ã€‚\")\n",
    "    print(\"è§£è¯»ï¼šè¯¥æ¨¡å‹å¯¹äº’åŠ¨ç‡çš„è§£é‡ŠåŠ›è¾ƒå¼ºï¼Œå¯ç”¨äºå†…å®¹é€‰é¢˜ä¸æŠ•æ”¾æ’æœŸçš„é‡åŒ–è¯„ä¼°ã€‚\")\n",
    "else:\n",
    "    print(\"[å†…å®¹é¢„æµ‹] æ•°æ®æˆ–ä¾èµ–ä¸è¶³ï¼Œæœªå®Œæˆå›å½’å»ºæ¨¡ã€‚\")\n",
    "\n",
    "if 'cluster' in df.columns:\n",
    "    vc = df['cluster'].value_counts()\n",
    "    print(f\"[å—ä¼—/å†…å®¹èšç±»] æœ€ä¼˜ç°‡æ•°ï¼š{len(vc)}ï¼Œç°‡è§„æ¨¡ï¼š{list(vc.values)}\")\n",
    "    print(\"è§£è¯»ï¼šç»“åˆå„ç°‡çš„æ•°å€¼å‡å€¼å·®å¼‚ï¼Œå¯ä¸ºä¸åŒäººç¾¤è¾“å‡ºå·®å¼‚åŒ–å†…å®¹ä¸æŠ•æ”¾æ—¶æ®µå»ºè®®ã€‚\")\n",
    "else:\n",
    "    print(\"[å—ä¼—/å†…å®¹èšç±»] æš‚æ— èšç±»ç»“æœã€‚\")\n",
    "\n",
    "if \"AdSpend\" in df.columns and \"ConversionRate\" in df.columns:\n",
    "    print(\"[å¹¿å‘ŠROI] å·²å®Œæˆ eCPA é¢„æµ‹ï¼Œå¯ç”¨äºä¸åŒæ¸ é“/åˆ›æ„çš„é¢„ç®—åˆ†é…ã€‚\")\n",
    "else:\n",
    "    print(\"[å¹¿å‘ŠROI] æ•°æ®ç¼ºå°‘æŠ•æ”¾å­—æ®µï¼Œå»ºè®®åœ¨åª’ä½“ç«¯è¡¥å……æ›å…‰ã€ç‚¹å‡»ã€è½¬åŒ–æ•°æ®åå†å»ºæ¨¡ã€‚\")\n",
    "\n",
    "print(\"â€”â€” ç­–ç•¥å»ºè®® â€”â€”\")\n",
    "print(\"1) å¹³å°å·®å¼‚ï¼šé«˜åŸºçº¿å¹³å°å¯è¿½æ±‚è§„æ¨¡ï¼Œä½åŸºçº¿å¹³å°è¿½æ±‚ROIï¼›ç»“åˆæ—¶é—´æ®µï¼ˆHour/Weekdayï¼‰çš„æ•æ„Ÿæ€§ç»“æœè¿›è¡Œæ’æœŸä¼˜åŒ–ã€‚\")\n",
    "print(\"2) å†…å®¹ç±»å‹ï¼šä¼˜å…ˆåˆ¶ä½œå¯¹äº’åŠ¨ç‡è´¡çŒ®åº¦é«˜çš„å†…å®¹ç±»å‹ï¼›å¿…è¦æ—¶åš A/B æµ‹è¯•éªŒè¯ã€‚\")\n",
    "print(\"3) å¹¿å‘ŠæŠ•æ”¾ï¼šè‹¥å¯ç”¨ï¼Œä½¿ç”¨ ROI æ¨¡å‹å¯¹ eCPA è¿›è¡Œé¢„æµ‹åˆ†å±‚ï¼Œé¢„ç®—å‘ä½ eCPAã€ç¨³å®šäººç¾¤å€¾æ–œã€‚\")\n",
    "print(\"4) å—ä¼—åˆ†ç¾¤ï¼šå¯¹æ¯ä¸ªç°‡åˆ¶å®šä¸åŒåˆ›æ„ä¸æŠ•æ”¾æ—¶æ®µï¼›å°†èšç±»æ ‡ç­¾å›å¡«è‡³BIç³»ç»ŸæŒç»­è·Ÿè¸ªè¡¨ç°ã€‚\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
